import { useState, useRef, useCallback } from 'react';

/**
 * ===========================================================================
 * Hook: useSpeechRecognition
 * 
 * åŠŸèƒ½ï¼š
 * 1. æµè§ˆå™¨è¯­éŸ³è¯†åˆ«ï¼ˆWeb Speech APIï¼‰
 * 2. å°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡æœ¬
 * 3. é€‚è€åŒ–è®¾è®¡ï¼Œç®€åŒ–ä½¿ç”¨
 * 
 * ä½¿ç”¨åœºæ™¯ï¼š
 * - è€äººç«¯æ‰€æœ‰è¾“å…¥æ¡†çš„è¯­éŸ³è¾“å…¥
 * - AI å¯¹è¯çš„è¯­éŸ³è¾“å…¥
 * - å¿ƒæƒ…è®°å½•çš„è¯­éŸ³å¤‡æ³¨
 * 
 * æµè§ˆå™¨æ”¯æŒï¼š
 * - Chromeã€Edgeã€Safariï¼ˆéƒ¨åˆ†ï¼‰
 * - éœ€è¦ HTTPS æˆ– localhost
 * ===========================================================================
 */

interface UseSpeechRecognitionReturn {
  /** æ˜¯å¦æ­£åœ¨ç›‘å¬ */
  isListening: boolean;
  /** è¯†åˆ«çš„æ–‡æœ¬ */
  transcript: string;
  /** å¼€å§‹ç›‘å¬ */
  startListening: () => void;
  /** åœæ­¢ç›‘å¬ */
  stopListening: () => void;
  /** æ¸…ç©ºè¯†åˆ«ç»“æœ */
  resetTranscript: () => void;
  /** æ˜¯å¦æ”¯æŒè¯­éŸ³è¯†åˆ« */
  isSupported: boolean;
}

export function useSpeechRecognition(): UseSpeechRecognitionReturn {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const recognitionRef = useRef<any>(null);

  /**
   * æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦æ”¯æŒè¯­éŸ³è¯†åˆ«
   */
  const isSupported = 
    'webkitSpeechRecognition' in window || 
    'SpeechRecognition' in window;

  /**
   * å¼€å§‹è¯­éŸ³è¯†åˆ«
   */
  const startListening = useCallback(() => {
    // æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ
    if (!isSupported) {
      alert('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½ï¼Œè¯·ä½¿ç”¨ Chrome æˆ– Edge æµè§ˆå™¨');
      return;
    }

    // å¦‚æœå·²ç»åœ¨ç›‘å¬ï¼Œå…ˆåœæ­¢
    if (isListening) {
      recognitionRef.current?.stop();
      setIsListening(false);
      return;
    }

    // åˆ›å»ºè¯­éŸ³è¯†åˆ«å®ä¾‹
    const SpeechRecognition = 
      (window as any).SpeechRecognition || 
      (window as any).webkitSpeechRecognition;
    
    const recognition = new SpeechRecognition();
    
    // é…ç½®
    recognition.lang = 'zh-CN';              // ä¸­æ–‡
    recognition.continuous = false;          // ä¸æŒç»­ç›‘å¬
    recognition.interimResults = false;      // ä¸è¿”å›ä¸­é—´ç»“æœ
    recognition.maxAlternatives = 1;         // åªè¿”å›æœ€ä½³ç»“æœ

    // äº‹ä»¶ç›‘å¬ - å¼€å§‹
    recognition.onstart = () => {
      console.log('ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ«');
      setIsListening(true);
    };

    // äº‹ä»¶ç›‘å¬ - ç»“æœ
    recognition.onresult = (event: any) => {
      const result = event.results[0][0].transcript;
      console.log('âœ… è¯†åˆ«ç»“æœ:', result);
      setTranscript(result);
      setIsListening(false);
    };

    // äº‹ä»¶ç›‘å¬ - é”™è¯¯
    recognition.onerror = (event: any) => {
      console.error('âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error);
      
      let errorMessage = 'è¯­éŸ³è¯†åˆ«å¤±è´¥';
      switch (event.error) {
        case 'no-speech':
          errorMessage = 'æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·å†è¯•ä¸€æ¬¡';
          break;
        case 'audio-capture':
          errorMessage = 'æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™';
          break;
        case 'not-allowed':
          errorMessage = 'éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸';
          break;
        case 'network':
          errorMessage = 'ç½‘ç»œé”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥';
          break;
      }
      
      alert(errorMessage);
      setIsListening(false);
    };

    // äº‹ä»¶ç›‘å¬ - ç»“æŸ
    recognition.onend = () => {
      console.log('ğŸ¤ è¯­éŸ³è¯†åˆ«ç»“æŸ');
      setIsListening(false);
    };

    // ä¿å­˜å®ä¾‹å¹¶å¼€å§‹è¯†åˆ«
    recognitionRef.current = recognition;
    recognition.start();
  }, [isListening, isSupported]);

  /**
   * åœæ­¢è¯­éŸ³è¯†åˆ«
   */
  const stopListening = useCallback(() => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      setIsListening(false);
    }
  }, []);

  /**
   * æ¸…ç©ºè¯†åˆ«ç»“æœ
   */
  const resetTranscript = useCallback(() => {
    setTranscript('');
  }, []);

  return {
    isListening,
    transcript,
    startListening,
    stopListening,
    resetTranscript,
    isSupported,
  };
}

/**
 * ä½¿ç”¨ç¤ºä¾‹ï¼š
 * 
 * ```typescript
 * function VoiceInput() {
 *   const [text, setText] = useState('');
 *   const { 
 *     isListening, 
 *     transcript, 
 *     startListening,
 *     resetTranscript,
 *     isSupported 
 *   } = useSpeechRecognition();
 * 
 *   // å½“è¯†åˆ«ç»“æœå˜åŒ–æ—¶ï¼Œæ›´æ–°è¾“å…¥æ¡†
 *   useEffect(() => {
 *     if (transcript) {
 *       setText(transcript);
 *       resetTranscript();
 *     }
 *   }, [transcript]);
 * 
 *   if (!isSupported) {
 *     return <p>æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¾“å…¥</p>;
 *   }
 * 
 *   return (
 *     <div>
 *       <Input value={text} onChange={(e) => setText(e.target.value)} />
 *       <Button onClick={startListening}>
 *         {isListening ? (
 *           <>
 *             <Mic className="animate-pulse" />
 *             æ­£åœ¨è†å¬...
 *           </>
 *         ) : (
 *           <>
 *             <Mic />
 *             è¯­éŸ³è¾“å…¥
 *           </>
 *         )}
 *       </Button>
 *     </div>
 *   );
 * }
 * ```
 */
