/**
 * å…¨åŒå·¥è¯­éŸ³åŠ©æ‰‹ç»„ä»¶
 * 
 * å‚è€ƒå°çº¢ä¹¦ FireRedChat å®ç°:
 * - å…¨åŒå·¥è¯­éŸ³äº¤äº’ï¼šæ”¯æŒç”¨æˆ·éšæ—¶æ‰“æ–­ AI
 * - è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰ï¼šæ™ºèƒ½æ£€æµ‹ç”¨æˆ·æ˜¯å¦åœ¨è¯´è¯
 * - æµå¼å“åº”ï¼šè¾¹è¯†åˆ«è¾¹å›å¤ï¼Œé™ä½å»¶è¿Ÿ
 * - çŠ¶æ€å¯è§†åŒ–ï¼šæ³¢å½¢åŠ¨ç”»ã€çŠ¶æ€æç¤º
 */

import React, { useState, useRef, useCallback, useEffect } from 'react';
import { Mic, MicOff, Volume2, VolumeX, Phone, PhoneOff, Loader2 } from 'lucide-react';
import { Button } from '../ui/button';
import { Card, CardContent } from '../ui/card';
import { useVoice } from '../../contexts/VoiceContext';

const API_BASE = 'http://localhost:8000';

// è¯­éŸ³çŠ¶æ€
type VoiceState = 
  | 'idle'           // ç©ºé—²
  | 'listening'      // æ­£åœ¨å¬ç”¨æˆ·è¯´è¯
  | 'processing'     // æ­£åœ¨å¤„ç†ç”¨æˆ·è¯­éŸ³
  | 'speaking'       // AIæ­£åœ¨è¯´è¯
  | 'interrupted';   // è¢«ç”¨æˆ·æ‰“æ–­

interface Message {
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
}

interface FullDuplexVoiceAssistantProps {
  onMessage?: (message: Message) => void;
  className?: string;
}

export function FullDuplexVoiceAssistant({ onMessage, className = '' }: FullDuplexVoiceAssistantProps) {
  // ä½¿ç”¨å…¨å±€è¯­éŸ³Contextï¼ˆé¿å…å¤šä¸ªè¯­éŸ³åŒæ—¶æ’­æ”¾ï¼‰
  const { speak: globalSpeak, stop: globalStop, isSpeaking: globalIsSpeaking } = useVoice();
  
  // çŠ¶æ€
  const [voiceState, setVoiceState] = useState<VoiceState>('idle');
  const [isSessionActive, setIsSessionActive] = useState(false);
  const [messages, setMessages] = useState<Message[]>([]);
  const [currentTranscript, setCurrentTranscript] = useState('');
  const [aiResponse, setAiResponse] = useState('');
  const [volumeLevel, setVolumeLevel] = useState(0);
  
  // Refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const recognitionRef = useRef<any>(null);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const isListeningRef = useRef(false);
  
  // VAD å‚æ•°
  const VAD_THRESHOLD = 0.02;  // éŸ³é‡é˜ˆå€¼
  const SILENCE_TIMEOUT = 1500; // é™éŸ³è¶…æ—¶ï¼ˆæ¯«ç§’ï¼‰
  
  /**
   * åˆå§‹åŒ–éŸ³é¢‘åˆ†æå™¨ï¼ˆç”¨äº VADï¼‰
   */
  const initAudioAnalyser = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      
      const audioContext = new AudioContext();
      audioContextRef.current = audioContext;
      
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      analyserRef.current = analyser;
      
      const source = audioContext.createMediaStreamSource(stream);
      source.connect(analyser);
      
      return true;
    } catch (error) {
      console.error('æ— æ³•è·å–éº¦å…‹é£æƒé™:', error);
      return false;
    }
  }, []);
  
  /**
   * æ£€æµ‹éŸ³é‡çº§åˆ«ï¼ˆVADï¼‰
   */
  const detectVolume = useCallback(() => {
    if (!analyserRef.current) return 0;
    
    const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
    analyserRef.current.getByteFrequencyData(dataArray);
    
    const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
    return average / 255; // å½’ä¸€åŒ–åˆ° 0-1
  }, []);
  
  /**
   * å¼€å§‹è¯­éŸ³ä¼šè¯
   */
  const startSession = useCallback(async () => {
    const success = await initAudioAnalyser();
    if (!success) return;
    
    setIsSessionActive(true);
    setVoiceState('listening');
    
    // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
    const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
    if (SpeechRecognition) {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'zh-CN';
      
      recognition.onresult = (event: any) => {
        let transcript = '';
        let isFinal = false;
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          transcript += event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            isFinal = true;
          }
        }
        
        setCurrentTranscript(transcript);
        
        // é‡ç½®é™éŸ³è®¡æ—¶å™¨
        if (silenceTimerRef.current) {
          clearTimeout(silenceTimerRef.current);
        }
        
        if (isFinal && transcript.trim()) {
          // ç”¨æˆ·è¯´å®Œäº†ï¼Œå¼€å§‹å¤„ç†
          handleUserSpeechEnd(transcript.trim());
        } else {
          // è®¾ç½®é™éŸ³æ£€æµ‹
          silenceTimerRef.current = setTimeout(() => {
            if (transcript.trim() && voiceState === 'listening') {
              handleUserSpeechEnd(transcript.trim());
            }
          }, SILENCE_TIMEOUT);
        }
      };
      
      recognition.onerror = (event: any) => {
        console.error('è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error);
        if (event.error !== 'no-speech') {
          // å°è¯•é‡å¯
          setTimeout(() => {
            if (isSessionActive && isListeningRef.current) {
              recognition.start();
            }
          }, 1000);
        }
      };
      
      recognition.onend = () => {
        // è‡ªåŠ¨é‡å¯
        if (isSessionActive && isListeningRef.current) {
          recognition.start();
        }
      };
      
      recognitionRef.current = recognition;
      recognition.start();
      isListeningRef.current = true;
    }
    
    // å¼€å§‹éŸ³é‡ç›‘æµ‹
    startVolumeMonitor();
    
    // æ’­æ”¾æ¬¢è¿è¯­
    await speak('æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„å¥åº·åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ');
  }, [initAudioAnalyser]);
  
  /**
   * ç»“æŸè¯­éŸ³ä¼šè¯
   */
  const endSession = useCallback(() => {
    setIsSessionActive(false);
    setVoiceState('idle');
    isListeningRef.current = false;
    
    // åœæ­¢è¯­éŸ³è¯†åˆ«
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
    
    // åœæ­¢å…¨å±€éŸ³é¢‘æ’­æ”¾
    globalStop();
    
    // æ¸…ç†
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
    }
    if (audioContextRef.current) {
      audioContextRef.current.close();
    }
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
    }
  }, [globalStop]);
  
  /**
   * éŸ³é‡ç›‘æµ‹å¾ªç¯
   */
  const startVolumeMonitor = useCallback(() => {
    const monitor = () => {
      if (!isSessionActive) return;
      
      const volume = detectVolume();
      setVolumeLevel(volume);
      
      requestAnimationFrame(monitor);
    };
    monitor();
  }, [detectVolume, isSessionActive]);
  
  /**
   * ç”¨æˆ·è¯´è¯ç»“æŸï¼Œå¤„ç†è¯­éŸ³
   */
  const handleUserSpeechEnd = useCallback(async (transcript: string) => {
    setVoiceState('processing');
    setCurrentTranscript('');
    
    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    const userMessage: Message = {
      role: 'user',
      content: transcript,
      timestamp: new Date()
    };
    setMessages(prev => [...prev, userMessage]);
    onMessage?.(userMessage);
    
    // è°ƒç”¨ AI è·å–å›å¤
    try {
      const response = await fetch(`${API_BASE}/api/v1/ai/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: transcript,
          context: 'health_assistant',
          stream: false
        })
      });
      
      if (!response.ok) throw new Error('AI è¯·æ±‚å¤±è´¥');
      
      const data = await response.json();
      const aiText = data.data?.response || data.response || 'æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„æ„æ€ã€‚';
      
      // æ·»åŠ  AI æ¶ˆæ¯
      const assistantMessage: Message = {
        role: 'assistant',
        content: aiText,
        timestamp: new Date()
      };
      setMessages(prev => [...prev, assistantMessage]);
      onMessage?.(assistantMessage);
      
      // æ’­æ”¾ AI å›å¤
      await speak(aiText);
      
    } catch (error) {
      console.error('AI è¯·æ±‚å¤±è´¥:', error);
      await speak('æŠ±æ­‰ï¼Œç³»ç»Ÿå‡ºç°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚');
    }
  }, [onMessage]);
  
  /**
   * TTS æ’­æ”¾ï¼ˆä½¿ç”¨å…¨å±€è¯­éŸ³Contextï¼Œæ”¯æŒæ‰“æ–­ï¼‰
   */
  const speak = useCallback(async (text: string) => {
    setVoiceState('speaking');
    setAiResponse(text);
    
    try {
      // ä½¿ç”¨å…¨å±€è¯­éŸ³æ’­æ”¾ï¼ˆä¼šè‡ªåŠ¨åœæ­¢å…¶ä»–æ­£åœ¨æ’­æ”¾çš„è¯­éŸ³ï¼‰
      await globalSpeak(text);
    } catch (error) {
      console.error('TTS å¤±è´¥:', error);
    }
    
    // å›åˆ°ç›‘å¬çŠ¶æ€
    if (isSessionActive) {
      setVoiceState('listening');
      setAiResponse('');
    }
  }, [isSessionActive, globalSpeak]);
  
  /**
   * æ‰‹åŠ¨æ‰“æ–­
   */
  const interrupt = useCallback(() => {
    globalStop();
    setVoiceState('listening');
    setAiResponse('');
  }, [globalStop]);
  
  // æ¸…ç†
  useEffect(() => {
    return () => {
      endSession();
    };
  }, [endSession]);
  
  // çŠ¶æ€æ–‡æœ¬
  const getStateText = () => {
    switch (voiceState) {
      case 'idle': return 'ç‚¹å‡»å¼€å§‹å¯¹è¯';
      case 'listening': return 'æ­£åœ¨è†å¬...';
      case 'processing': return 'æ­£åœ¨æ€è€ƒ...';
      case 'speaking': return 'æ­£åœ¨å›å¤...';
      case 'interrupted': return 'å·²æ‰“æ–­';
      default: return '';
    }
  };
  
  // çŠ¶æ€é¢œè‰²
  const getStateColor = () => {
    switch (voiceState) {
      case 'listening': return 'bg-green-500';
      case 'processing': return 'bg-yellow-500';
      case 'speaking': return 'bg-blue-500';
      case 'interrupted': return 'bg-orange-500';
      default: return 'bg-gray-400';
    }
  };

  return (
    <Card className={`${className} overflow-hidden`}>
      <CardContent className="p-6">
        {/* çŠ¶æ€æŒ‡ç¤ºå™¨ */}
        <div className="flex items-center justify-center mb-6">
          <div className="relative">
            {/* æ³¢å½¢åŠ¨ç”» */}
            <div className={`w-32 h-32 rounded-full ${getStateColor()} transition-all duration-300 flex items-center justify-center`}
                 style={{
                   transform: `scale(${1 + volumeLevel * 0.3})`,
                   boxShadow: voiceState === 'listening' 
                     ? `0 0 ${volumeLevel * 60}px ${volumeLevel * 30}px rgba(34, 197, 94, 0.3)`
                     : voiceState === 'speaking'
                     ? '0 0 30px 15px rgba(59, 130, 246, 0.3)'
                     : 'none'
                 }}>
              {voiceState === 'processing' ? (
                <Loader2 className="w-12 h-12 text-white animate-spin" />
              ) : voiceState === 'speaking' ? (
                <Volume2 className="w-12 h-12 text-white animate-pulse" />
              ) : isSessionActive ? (
                <Mic className="w-12 h-12 text-white" />
              ) : (
                <MicOff className="w-12 h-12 text-white" />
              )}
            </div>
            
            {/* éŸ³é‡æ¡ */}
            {isSessionActive && voiceState === 'listening' && (
              <div className="absolute -bottom-4 left-1/2 transform -translate-x-1/2 flex gap-1">
                {[...Array(5)].map((_, i) => (
                  <div
                    key={i}
                    className="w-2 bg-green-500 rounded-full transition-all duration-75"
                    style={{
                      height: `${Math.min(32, 8 + volumeLevel * 100 * (i + 1) / 5)}px`
                    }}
                  />
                ))}
              </div>
            )}
          </div>
        </div>
        
        {/* çŠ¶æ€æ–‡æœ¬ */}
        <div className="text-center mb-4">
          <p className="text-xl font-medium text-gray-700">{getStateText()}</p>
          {currentTranscript && (
            <p className="mt-2 text-lg text-gray-600 italic">"{currentTranscript}"</p>
          )}
          {aiResponse && voiceState === 'speaking' && (
            <p className="mt-2 text-lg text-blue-600">"{aiResponse}"</p>
          )}
        </div>
        
        {/* æ§åˆ¶æŒ‰é’® */}
        <div className="flex justify-center gap-4">
          {!isSessionActive ? (
            <Button
              size="lg"
              className="rounded-full w-16 h-16 bg-green-500 hover:bg-green-600"
              onClick={startSession}
            >
              <Phone className="w-8 h-8" />
            </Button>
          ) : (
            <>
              {voiceState === 'speaking' && (
                <Button
                  size="lg"
                  variant="outline"
                  className="rounded-full w-14 h-14"
                  onClick={interrupt}
                >
                  <VolumeX className="w-6 h-6" />
                </Button>
              )}
              <Button
                size="lg"
                className="rounded-full w-16 h-16 bg-red-500 hover:bg-red-600"
                onClick={endSession}
              >
                <PhoneOff className="w-8 h-8" />
              </Button>
            </>
          )}
        </div>
        
        {/* å¯¹è¯å†å² */}
        {messages.length > 0 && (
          <div className="mt-6 max-h-48 overflow-y-auto space-y-2">
            {messages.slice(-6).map((msg, index) => (
              <div
                key={index}
                className={`p-3 rounded-lg text-sm ${
                  msg.role === 'user'
                    ? 'bg-green-100 text-green-800 ml-8'
                    : 'bg-blue-100 text-blue-800 mr-8'
                }`}
              >
                <span className="font-medium">{msg.role === 'user' ? 'æ‚¨ï¼š' : 'AIï¼š'}</span>
                {msg.content}
              </div>
            ))}
          </div>
        )}
        
        {/* ä½¿ç”¨æç¤º */}
        <div className="mt-4 text-center text-sm text-gray-500">
          {isSessionActive ? (
            <p>ğŸ’¡ æ‚¨å¯ä»¥éšæ—¶è¯´è¯æ‰“æ–­ AI çš„å›å¤</p>
          ) : (
            <p>ç‚¹å‡»ç»¿è‰²æŒ‰é’®å¼€å§‹è¯­éŸ³å¯¹è¯</p>
          )}
        </div>
      </CardContent>
    </Card>
  );
}

export default FullDuplexVoiceAssistant;
